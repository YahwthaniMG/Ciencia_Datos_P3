{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lisis Automatizado de Datos con IA\n",
    "## Versi√≥n 1: An√°lisis Exploratorio + Generaci√≥n de Insights con Claude\n",
    "\n",
    "**Este notebook analiza autom√°ticamente cualquier archivo CSV y genera un reporte ejecutivo usando la API de Claude.**\n",
    "\n",
    "### Requisitos del dataset:\n",
    "- M√≠nimo 2,000 registros\n",
    "- M√≠nimo 10 columnas\n",
    "- Puede contener variables num√©ricas y categ√≥ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CONFIGURACI√ìN INICIAL\n",
    "### üëá MODIFICA SOLO ESTA CELDA üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACI√ìN - MODIFICA ESTOS VALORES\n",
    "# ============================================================================\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "CSV_PATH = \"data/tu_archivo.csv\"\n",
    "\n",
    "# Descripci√≥n breve del dataset (para que Claude entienda el contexto)\n",
    "DATASET_DESCRIPTION = \"\"\"\n",
    "Describe aqu√≠ tu dataset en 2-3 oraciones.\n",
    "Por ejemplo: \"Dataset de calidad de vinos blancos con propiedades fisicoqu√≠micas.\n",
    "La variable objetivo es 'quality' que indica la calidad del vino en escala del 1-10.\"\n",
    "\"\"\"\n",
    "\n",
    "# Nombre del dataset (para los reportes)\n",
    "DATASET_NAME = \"Mi Dataset\"\n",
    "\n",
    "# Variable objetivo (opcional - dejar None para detecci√≥n autom√°tica)\n",
    "# Si tu dataset tiene una variable objetivo espec√≠fica, ponla aqu√≠\n",
    "TARGET_VARIABLE = None  # Ejemplo: \"quality\" o \"precio\" o None\n",
    "\n",
    "# Separador del CSV (coma por defecto)\n",
    "CSV_SEPARATOR = \",\"\n",
    "\n",
    "# ============================================================================\n",
    "print(\" Configuraci√≥n cargada\")\n",
    "print(f\"    Archivo: {CSV_PATH}\")\n",
    "print(f\"    Dataset: {DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1- Importar Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuraci√≥n\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Crear directorios\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "print(\"Librer√≠as cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2- Cargar y Validar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "print(\"Cargando dataset...\")\n",
    "df = pd.read_csv(CSV_PATH, sep=CSV_SEPARATOR)\n",
    "\n",
    "# Limpiar columnas vac√≠as\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape[0]:,} registros x {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaci√≥n de requisitos\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDACI√ìN DEL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "requisitos_cumplidos = True\n",
    "\n",
    "# Verificar registros\n",
    "if df.shape[0] >= 2000:\n",
    "    print(f\"Registros: {df.shape[0]:,} (m√≠nimo 2,000)\")\n",
    "else:\n",
    "    print(f\"Registros: {df.shape[0]:,} (m√≠nimo 2,000)\")\n",
    "    requisitos_cumplidos = False\n",
    "\n",
    "# Verificar columnas\n",
    "if df.shape[1] >= 10:\n",
    "    print(f\"Columnas: {df.shape[1]} (m√≠nimo 10)\")\n",
    "else:\n",
    "    print(f\"Columnas: {df.shape[1]} (m√≠nimo 10)\")\n",
    "    requisitos_cumplidos = False\n",
    "\n",
    "if requisitos_cumplidos:\n",
    "    print(\"\\n¬°Dataset v√°lido! Continuando con el an√°lisis...\")\n",
    "else:\n",
    "    print(\"\\nEl dataset no cumple con los requisitos m√≠nimos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar tipos de columnas autom√°ticamente\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IDENTIFICACI√ìN AUTOM√ÅTICA DE VARIABLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Columnas num√©ricas\n",
    "columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Columnas categ√≥ricas (object, category, o num√©ricas con pocos valores √∫nicos)\n",
    "columnas_categoricas = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Detectar num√©ricas que podr√≠an ser categ√≥ricas (pocos valores √∫nicos)\n",
    "for col in columnas_numericas.copy():\n",
    "    if df[col].nunique() <= 10:  # Si tiene 10 o menos valores √∫nicos\n",
    "        columnas_categoricas.append(col)\n",
    "        \n",
    "# Variables continuas (num√©ricas que no son categ√≥ricas)\n",
    "columnas_continuas = [col for col in columnas_numericas if col not in columnas_categoricas]\n",
    "\n",
    "print(f\"\\nVariables num√©ricas continuas ({len(columnas_continuas)}):\")\n",
    "for col in columnas_continuas:\n",
    "    print(f\"   ‚Ä¢ {col}\")\n",
    "\n",
    "print(f\"\\nVariables categ√≥ricas ({len(columnas_categoricas)}):\")\n",
    "for col in columnas_categoricas:\n",
    "    print(f\"   ‚Ä¢ {col} ({df[col].nunique()} valores √∫nicos)\")\n",
    "\n",
    "# Detectar variable objetivo\n",
    "if TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\n",
    "    variable_objetivo = TARGET_VARIABLE\n",
    "elif columnas_categoricas:\n",
    "    # Usar la √∫ltima columna categ√≥rica como objetivo por defecto\n",
    "    variable_objetivo = columnas_categoricas[-1]\n",
    "else:\n",
    "    variable_objetivo = None\n",
    "\n",
    "print(f\"\\nVariable objetivo detectada: {variable_objetivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vista previa del dataset\n",
    "print(\"\\nVista previa del dataset:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n del dataset\n",
    "print(\"\\nInformaci√≥n del dataset:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3- An√°lisis de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de valores faltantes\n",
    "print(\"=\"*60)\n",
    "print(\"AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "valores_faltantes = df.isnull().sum()\n",
    "porcentaje_faltantes = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "df_faltantes = pd.DataFrame({\n",
    "    'Columna': valores_faltantes.index,\n",
    "    'Faltantes': valores_faltantes.values,\n",
    "    'Porcentaje (%)': porcentaje_faltantes.values\n",
    "}).sort_values('Faltantes', ascending=False)\n",
    "\n",
    "print(f\"\\nTotal de celdas: {df.shape[0] * df.shape[1]:,}\")\n",
    "print(f\"Celdas con valores faltantes: {df.isnull().sum().sum():,}\")\n",
    "print(f\"Porcentaje promedio de faltantes: {porcentaje_faltantes.mean():.2f}%\")\n",
    "\n",
    "# Mostrar solo columnas con faltantes\n",
    "df_con_faltantes = df_faltantes[df_faltantes['Faltantes'] > 0]\n",
    "if len(df_con_faltantes) > 0:\n",
    "    print(\"\\nColumnas con valores faltantes:\")\n",
    "    display(df_con_faltantes)\n",
    "else:\n",
    "    print(\"\\nNo hay valores faltantes en el dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de valores faltantes\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis', ax=ax)\n",
    "ax.set_title(f'Mapa de Valores Faltantes - {DATASET_NAME}', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Variables')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/01_heatmap_valores_faltantes.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Guardado: outputs/01_heatmap_valores_faltantes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4- An√°lisis Estad√≠stico Descriptivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas para variables num√©ricas\n",
    "print(\"=\"*60)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if columnas_continuas:\n",
    "    estadisticas = df[columnas_continuas].describe().T\n",
    "    estadisticas['mediana'] = df[columnas_continuas].median()\n",
    "    estadisticas['moda'] = df[columnas_continuas].mode().iloc[0]\n",
    "    estadisticas['varianza'] = df[columnas_continuas].var()\n",
    "    estadisticas['rango'] = estadisticas['max'] - estadisticas['min']\n",
    "    estadisticas['coef_var (%)'] = (estadisticas['std'] / estadisticas['mean']) * 100\n",
    "    \n",
    "    display(estadisticas.round(4))\n",
    "else:\n",
    "    print(\"No hay variables num√©ricas continuas para analizar.\")\n",
    "    estadisticas = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas para variables categ√≥ricas\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES CATEG√ìRICAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "resumen_categoricas = {}\n",
    "\n",
    "for col in columnas_categoricas:\n",
    "    frecuencias = df[col].value_counts()\n",
    "    porcentajes = (frecuencias / len(df) * 100).round(2)\n",
    "    \n",
    "    resumen_categoricas[col] = {\n",
    "        'valores_unicos': df[col].nunique(),\n",
    "        'moda': df[col].mode()[0] if len(df[col].mode()) > 0 else None,\n",
    "        'frecuencia_moda': frecuencias.iloc[0] if len(frecuencias) > 0 else 0,\n",
    "        'porcentaje_moda': porcentajes.iloc[0] if len(porcentajes) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"   Valores √∫nicos: {df[col].nunique()}\")\n",
    "    print(f\"   Moda: {resumen_categoricas[col]['moda']} ({resumen_categoricas[col]['porcentaje_moda']}%)\")\n",
    "    print(f\"   Distribuci√≥n:\")\n",
    "    for val, freq, pct in zip(frecuencias.index[:5], frecuencias.values[:5], porcentajes.values[:5]):\n",
    "        print(f\"      ‚Ä¢ {val}: {freq:,} ({pct}%)\")\n",
    "    if len(frecuencias) > 5:\n",
    "        print(f\"      ... y {len(frecuencias) - 5} valores m√°s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5- Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas para variables num√©ricas\n",
    "if columnas_continuas:\n",
    "    n_cols = 3\n",
    "    n_rows = (len(columnas_continuas) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 and n_cols == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(columnas_continuas):\n",
    "        ax = axes[i]\n",
    "        ax.hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        ax.axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Media: {df[col].mean():.2f}')\n",
    "        ax.axvline(df[col].median(), color='green', linestyle=':', linewidth=2, label=f'Mediana: {df[col].median():.2f}')\n",
    "        ax.set_title(col, fontsize=11, fontweight='bold')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    # Ocultar ejes vac√≠os\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Histogramas - {DATASET_NAME}', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/02_histogramas.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Guardado: outputs/02_histogramas.png\")\n",
    "else:\n",
    "    print(\"No hay variables num√©ricas para crear histogramas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para variables num√©ricas\n",
    "if columnas_continuas:\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 and n_cols == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(columnas_continuas):\n",
    "        ax = axes[i]\n",
    "        bp = ax.boxplot(df[col].dropna(), patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        ax.set_title(col, fontsize=11, fontweight='bold')\n",
    "    \n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Boxplots (Detecci√≥n de Outliers) - {DATASET_NAME}', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/03_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Guardado: outputs/03_boxplots.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°ficos de barras para variables categ√≥ricas\n",
    "if columnas_categoricas:\n",
    "    n_cat = min(len(columnas_categoricas), 6)  # M√°ximo 6 gr√°ficos\n",
    "    n_cols_cat = min(3, n_cat)\n",
    "    n_rows_cat = (n_cat + n_cols_cat - 1) // n_cols_cat\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows_cat, n_cols_cat, figsize=(15, 4*n_rows_cat))\n",
    "    if n_cat == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(columnas_categoricas[:n_cat]):\n",
    "        ax = axes[i]\n",
    "        frecuencias = df[col].value_counts().head(10)  # Top 10 valores\n",
    "        colores = sns.color_palette('husl', n_colors=len(frecuencias))\n",
    "        \n",
    "        bars = ax.bar(frecuencias.index.astype(str), frecuencias.values, color=colores, edgecolor='black')\n",
    "        ax.set_title(col, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Frecuencia')\n",
    "        plt.sca(ax)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Distribuci√≥n de Variables Categ√≥ricas - {DATASET_NAME}', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/04_barras_categoricas.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Guardado: outputs/04_barras_categoricas.png\")\n",
    "else:\n",
    "    print(\"No hay variables categ√≥ricas para crear gr√°ficos de barras.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6- Detecci√≥n de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecci√≥n de outliers usando m√©todo IQR\n",
    "print(\"=\"*60)\n",
    "print(\"DETECCI√ìN DE OUTLIERS (M√©todo IQR)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def contar_outliers_iqr(columna):\n",
    "    Q1 = columna.quantile(0.25)\n",
    "    Q3 = columna.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    outliers = ((columna < limite_inferior) | (columna > limite_superior)).sum()\n",
    "    return outliers, limite_inferior, limite_superior\n",
    "\n",
    "resultados_outliers = []\n",
    "total_outliers = 0\n",
    "\n",
    "for col in columnas_continuas:\n",
    "    n_outliers, lim_inf, lim_sup = contar_outliers_iqr(df[col].dropna())\n",
    "    pct_outliers = (n_outliers / len(df)) * 100\n",
    "    total_outliers += n_outliers\n",
    "    \n",
    "    resultados_outliers.append({\n",
    "        'Variable': col,\n",
    "        'Outliers': n_outliers,\n",
    "        'Porcentaje (%)': pct_outliers,\n",
    "        'L√≠mite Inf.': lim_inf,\n",
    "        'L√≠mite Sup.': lim_sup\n",
    "    })\n",
    "\n",
    "df_outliers = pd.DataFrame(resultados_outliers).sort_values('Outliers', ascending=False)\n",
    "print(f\"\\nTotal de outliers detectados: {total_outliers:,}\")\n",
    "display(df_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7- An√°lisis de Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlaci√≥n\n",
    "if len(columnas_numericas) > 1:\n",
    "    print(\"=\"*60)\n",
    "    print(\"AN√ÅLISIS DE CORRELACIONES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    matriz_correlacion = df[columnas_numericas].corr(method='pearson')\n",
    "    \n",
    "    # Heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(matriz_correlacion, dtype=bool))\n",
    "    sns.heatmap(matriz_correlacion, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "                center=0, square=True, linewidths=0.5, ax=ax, cbar_kws={'shrink': 0.8})\n",
    "    ax.set_title(f'Matriz de Correlaciones - {DATASET_NAME}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/05_heatmap_correlaciones.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Guardado: outputs/05_heatmap_correlaciones.png\")\n",
    "else:\n",
    "    print(\"No hay suficientes variables num√©ricas para an√°lisis de correlaciones.\")\n",
    "    matriz_correlacion = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlaciones\n",
    "if len(columnas_numericas) > 1:\n",
    "    correlaciones_pares = []\n",
    "    for i in range(len(matriz_correlacion.columns)):\n",
    "        for j in range(i+1, len(matriz_correlacion.columns)):\n",
    "            col1 = matriz_correlacion.columns[i]\n",
    "            col2 = matriz_correlacion.columns[j]\n",
    "            corr = matriz_correlacion.iloc[i, j]\n",
    "            correlaciones_pares.append((col1, col2, corr, abs(corr)))\n",
    "    \n",
    "    correlaciones_pares.sort(key=lambda x: x[3], reverse=True)\n",
    "    top_correlaciones = correlaciones_pares[:10]\n",
    "    \n",
    "    print(\"\\nTOP 10 CORRELACIONES M√ÅS FUERTES:\")\n",
    "    df_top_corr = pd.DataFrame(top_correlaciones, columns=['Variable 1', 'Variable 2', 'Correlaci√≥n', '|Correlaci√≥n|'])\n",
    "    df_top_corr.index = range(1, len(df_top_corr) + 1)\n",
    "    display(df_top_corr)\n",
    "else:\n",
    "    top_correlaciones = []\n",
    "    df_top_corr = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlaciones con variable objetivo (si existe)\n",
    "if variable_objetivo and variable_objetivo in columnas_numericas:\n",
    "    print(f\"\\nCORRELACIONES CON VARIABLE OBJETIVO ({variable_objetivo}):\")\n",
    "    corr_con_objetivo = matriz_correlacion[variable_objetivo].drop(variable_objetivo).sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    df_corr_objetivo = pd.DataFrame({\n",
    "        'Variable': corr_con_objetivo.index,\n",
    "        'Correlaci√≥n': corr_con_objetivo.values\n",
    "    })\n",
    "    df_corr_objetivo.index = range(1, len(df_corr_objetivo) + 1)\n",
    "    display(df_corr_objetivo)\n",
    "    \n",
    "    # Gr√°fico de barras\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = ['green' if c > 0 else 'red' for c in corr_con_objetivo.values]\n",
    "    ax.barh(corr_con_objetivo.index, corr_con_objetivo.values, color=colors, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    ax.set_xlabel('Correlaci√≥n')\n",
    "    ax.set_title(f'Correlaci√≥n con {variable_objetivo}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/06_correlacion_objetivo.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Guardado: outputs/06_correlacion_objetivo.png\")\n",
    "else:\n",
    "    df_corr_objetivo = pd.DataFrame()\n",
    "    print(\"\\nNo se detect√≥ variable objetivo num√©rica para an√°lisis de correlaci√≥n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8- Preparar Resumen para Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir resumen completo del an√°lisis\n",
    "print(\"=\"*60)\n",
    "print(\"GENERANDO RESUMEN DEL AN√ÅLISIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Resumen de variable objetivo si existe\n",
    "resumen_objetivo = \"\"\n",
    "if variable_objetivo:\n",
    "    if variable_objetivo in columnas_categoricas:\n",
    "        freq_obj = df[variable_objetivo].value_counts()\n",
    "        pct_obj = (freq_obj / len(df) * 100).round(2)\n",
    "        resumen_objetivo = f\"\"\"\n",
    "   Variable objetivo: {variable_objetivo}\n",
    "   Tipo: Categ√≥rica\n",
    "   Valores √∫nicos: {df[variable_objetivo].nunique()}\n",
    "   Distribuci√≥n:\n",
    "{chr(10).join([f'      - {val}: {freq:,} ({pct}%)' for val, freq, pct in zip(freq_obj.index[:10], freq_obj.values[:10], pct_obj.values[:10])])}\n",
    "\"\"\"\n",
    "    else:\n",
    "        resumen_objetivo = f\"\"\"\n",
    "   Variable objetivo: {variable_objetivo}\n",
    "   Tipo: Num√©rica\n",
    "   Media: {df[variable_objetivo].mean():.4f}\n",
    "   Mediana: {df[variable_objetivo].median():.4f}\n",
    "   Std: {df[variable_objetivo].std():.4f}\n",
    "   Rango: [{df[variable_objetivo].min():.4f}, {df[variable_objetivo].max():.4f}]\n",
    "\"\"\"\n",
    "\n",
    "resumen_analisis = f\"\"\"\n",
    "================================================================================\n",
    "RESUMEN DEL AN√ÅLISIS DE DATOS\n",
    "================================================================================\n",
    "\n",
    "DESCRIPCI√ìN DEL DATASET:\n",
    "{DATASET_DESCRIPTION}\n",
    "\n",
    "1. INFORMACI√ìN GENERAL\n",
    "--------------------------------------------------------------------------------\n",
    "   - Nombre: {DATASET_NAME}\n",
    "   - Dimensiones: {df.shape[0]:,} registros x {df.shape[1]} columnas\n",
    "   - Variables num√©ricas continuas: {len(columnas_continuas)}\n",
    "   - Variables categ√≥ricas: {len(columnas_categoricas)}\n",
    "   - Duplicados: {df.duplicated().sum():,}\n",
    "\n",
    "2. CALIDAD DE DATOS\n",
    "--------------------------------------------------------------------------------\n",
    "   - Valores faltantes totales: {df.isnull().sum().sum():,} ({porcentaje_faltantes.mean():.2f}% promedio)\n",
    "   - Columnas con faltantes: {(valores_faltantes > 0).sum()}\n",
    "   - Total de outliers (IQR): {total_outliers:,}\n",
    "\n",
    "3. VARIABLE OBJETIVO\n",
    "--------------------------------------------------------------------------------\n",
    "{resumen_objetivo if resumen_objetivo else '   No se identific√≥ variable objetivo espec√≠fica.'}\n",
    "\n",
    "4. ESTAD√çSTICAS DESCRIPTIVAS (Variables Num√©ricas)\n",
    "--------------------------------------------------------------------------------\n",
    "{estadisticas[['mean', 'std', 'min', 'max', 'mediana']].to_string() if len(estadisticas) > 0 else 'No hay variables num√©ricas.'}\n",
    "\n",
    "5. VARIABLES CATEG√ìRICAS\n",
    "--------------------------------------------------------------------------------\n",
    "{chr(10).join([f\"   - {col}: {info['valores_unicos']} valores √∫nicos, moda: {info['moda']} ({info['porcentaje_moda']}%)\" for col, info in resumen_categoricas.items()])}\n",
    "\n",
    "6. OUTLIERS DETECTADOS (Top 5)\n",
    "--------------------------------------------------------------------------------\n",
    "{df_outliers.head(5).to_string() if len(df_outliers) > 0 else 'No hay outliers detectados.'}\n",
    "\n",
    "7. TOP 5 CORRELACIONES M√ÅS FUERTES\n",
    "--------------------------------------------------------------------------------\n",
    "{df_top_corr.head(5).to_string() if len(df_top_corr) > 0 else 'No hay suficientes variables para correlaciones.'}\n",
    "\n",
    "8. CORRELACIONES CON VARIABLE OBJETIVO\n",
    "--------------------------------------------------------------------------------\n",
    "{df_corr_objetivo.to_string() if len(df_corr_objetivo) > 0 else 'No aplica o no hay variable objetivo num√©rica.'}\n",
    "\n",
    "9. OBSERVACIONES INICIALES\n",
    "--------------------------------------------------------------------------------\n",
    "   - El dataset tiene {df.shape[0]:,} registros y {df.shape[1]} variables\n",
    "   - {'Hay ' + str(df.isnull().sum().sum()) + ' valores faltantes que requieren tratamiento' if df.isnull().sum().sum() > 0 else 'No hay valores faltantes'}\n",
    "   - Se detectaron {total_outliers:,} outliers en las variables num√©ricas\n",
    "   - {'La correlaci√≥n m√°s fuerte es entre ' + top_correlaciones[0][0] + ' y ' + top_correlaciones[0][1] + ' (' + str(round(top_correlaciones[0][2], 3)) + ')' if top_correlaciones else 'No se calcularon correlaciones'}\n",
    "\"\"\"\n",
    "\n",
    "print(resumen_analisis)\n",
    "\n",
    "# Guardar resumen\n",
    "with open('reports/resumen_analisis.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(resumen_analisis)\n",
    "print(\"\\nResumen guardado en: reports/resumen_analisis.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9- Generaci√≥n de Insights con Claude API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar cliente de Anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"ERROR: No se encontr√≥ ANTHROPIC_API_KEY\")\n",
    "    print(\"   Por favor, configura tu API key en el archivo .env\")\n",
    "else:\n",
    "    print(\"API Key encontrada\")\n",
    "    print(f\"   Key: {api_key[:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para generar insights con Claude\n",
    "def generar_insights_con_claude(resumen, descripcion, nombre_dataset, api_key):\n",
    "    \"\"\"\n",
    "    Env√≠a el resumen del an√°lisis a Claude y obtiene insights profesionales.\n",
    "    \"\"\"\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Eres un experto en an√°lisis de datos y ciencia de datos. \n",
    "A continuaci√≥n te presento el resumen de un an√°lisis exploratorio de datos.\n",
    "\n",
    "CONTEXTO DEL DATASET:\n",
    "{descripcion}\n",
    "\n",
    "Por favor, genera un REPORTE EJECUTIVO PROFESIONAL que incluya:\n",
    "\n",
    "1. RESUMEN EJECUTIVO (2-3 p√°rrafos)\n",
    "   - Descripci√≥n general del dataset\n",
    "   - Calidad de los datos\n",
    "   - Principales caracter√≠sticas observadas\n",
    "\n",
    "2. HALLAZGOS CLAVE (3-5 hallazgos)\n",
    "   - Insights importantes descubiertos en el an√°lisis\n",
    "   - Patrones o tendencias relevantes\n",
    "   - Relaciones significativas entre variables\n",
    "\n",
    "3. CALIDAD DE LOS DATOS\n",
    "   - Evaluaci√≥n de valores faltantes\n",
    "   - An√°lisis de outliers detectados\n",
    "   - Recomendaciones de limpieza\n",
    "\n",
    "4. RECOMENDACIONES DE PREPROCESAMIENTO (3-5 recomendaciones)\n",
    "   - Sugerencias para mejorar la calidad de los datos\n",
    "   - T√©cnicas recomendadas para manejar outliers\n",
    "   - Transformaciones sugeridas para modelado\n",
    "\n",
    "5. CONCLUSIONES Y PR√ìXIMOS PASOS\n",
    "   - Resumen de conclusiones principales\n",
    "   - Recomendaciones para an√°lisis futuros\n",
    "   - Posibles aplicaciones del an√°lisis\n",
    "\n",
    "DATOS DEL AN√ÅLISIS:\n",
    "{resumen}\n",
    "\n",
    "Por favor, responde en espa√±ol y de manera profesional. Usa los datos espec√≠ficos \n",
    "del resumen para fundamentar cada punto. S√© espec√≠fico y evita generalidades.\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=4000,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar insights con Claude\n",
    "if api_key:\n",
    "    print(\"Generando insights con Claude...\")\n",
    "    print(\"   (Esto puede tomar unos segundos)\\n\")\n",
    "    \n",
    "    try:\n",
    "        insights = generar_insights_con_claude(\n",
    "            resumen_analisis, \n",
    "            DATASET_DESCRIPTION, \n",
    "            DATASET_NAME, \n",
    "            api_key\n",
    "        )\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"REPORTE EJECUTIVO GENERADO POR CLAUDE\")\n",
    "        print(\"=\"*70)\n",
    "        print(insights)\n",
    "        \n",
    "        # Guardar insights\n",
    "        with open('reports/insights_claude.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"REPORTE EJECUTIVO - {DATASET_NAME}\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(insights)\n",
    "        \n",
    "        print(\"\\nInsights guardados en: reports/insights_claude.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar insights: {e}\")\n",
    "else:\n",
    "    print(\"No se puede generar insights sin API key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final del an√°lisis\n",
    "print(\"=\"*70)\n",
    "print(\"AN√ÅLISIS COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET: {DATASET_NAME}\n",
    "ARCHIVO: {CSV_PATH}\n",
    "DIMENSIONES: {df.shape[0]:,} registros x {df.shape[1]} columnas\n",
    "\n",
    "   ARCHIVOS GENERADOS:\n",
    "   Visualizaciones (outputs/):\n",
    "   ‚Ä¢ 01_heatmap_valores_faltantes.png\n",
    "   ‚Ä¢ 02_histogramas.png\n",
    "   ‚Ä¢ 03_boxplots.png\n",
    "   ‚Ä¢ 04_barras_categoricas.png\n",
    "   ‚Ä¢ 05_heatmap_correlaciones.png\n",
    "   ‚Ä¢ 06_correlacion_objetivo.png\n",
    "   \n",
    "   Reportes (reports/):\n",
    "   ‚Ä¢ resumen_analisis.txt\n",
    "   ‚Ä¢ insights_claude.txt\n",
    "\n",
    "   M√âTRICAS CLAVE:\n",
    "   ‚Ä¢ Variables num√©ricas: {len(columnas_continuas)}\n",
    "   ‚Ä¢ Variables categ√≥ricas: {len(columnas_categoricas)}\n",
    "   ‚Ä¢ Valores faltantes: {df.isnull().sum().sum():,}\n",
    "   ‚Ä¢ Outliers detectados: {total_outliers:,}\n",
    "   ‚Ä¢ Variable objetivo: {variable_objetivo}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n¬°An√°lisis completado exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
