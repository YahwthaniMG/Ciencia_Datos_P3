{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Automatizado Avanzado de Calidad de Vinos Blancos\n",
    "\n",
    "**Proyecto de Minería de Datos - Análisis Exhaustivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ANÁLISIS AUTOMATIZADO AVANZADO DE CALIDAD DE VINOS BLANCOS\n",
    "Proyecto de Minería de Datos\n",
    "\n",
    "Este script realiza:\n",
    "- Análisis exploratorio completo\n",
    "- Tests estadísticos\n",
    "- Machine Learning\n",
    "- Clustering y PCA\n",
    "- Generación de insights con Claude API\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANÁLISIS AUTOMATIZADO DE CALIDAD DE VINOS BLANCOS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, normaltest, f_oneway, kruskal\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                             precision_score, recall_score, f1_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score,\n",
    "                             silhouette_score)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configuración\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "load_dotenv()\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Crear directorios\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISIS AUTOMATIZADO DE CALIDAD DE VINOS BLANCOS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CARGAR Y VALIDAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/16] Cargando datos...\n",
      "   Dataset: 4,898 registros x 12 columnas\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/16] Cargando datos...\")\n",
    "df = pd.read_csv('data/winequality-white.csv')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "info_dataset = {\n",
    "    'Nombre': 'Wine Quality - White Wine',\n",
    "    'Registros': df.shape[0],\n",
    "    'Columnas': df.shape[1],\n",
    "    'Duplicados': df.duplicated().sum()\n",
    "}\n",
    "\n",
    "columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "columnas_continuas = [col for col in columnas_numericas if col != 'quality']\n",
    "\n",
    "print(f\"   Dataset: {info_dataset['Registros']:,} registros x {info_dataset['Columnas']} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ANÁLISIS DE VALORES FALTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/16] Analizando valores faltantes...\n"
     ]
    }
   ],
   "source": [
    "print(\"[2/16] Analizando valores faltantes...\")\n",
    "valores_faltantes = df.isnull().sum()\n",
    "porcentaje_faltantes = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Heatmap de valores faltantes\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis', ax=ax)\n",
    "ax.set_title('Mapa de Valores Faltantes', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/01_heatmap_valores_faltantes_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ESTADÍSTICAS DESCRIPTIVAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/16] Calculando estadísticas descriptivas...\n"
     ]
    }
   ],
   "source": [
    "print(\"[3/16] Calculando estadísticas descriptivas...\")\n",
    "estadisticas = df.describe().T\n",
    "estadisticas['mediana'] = df.median()\n",
    "estadisticas['moda'] = df.mode().iloc[0]\n",
    "estadisticas['varianza'] = df.var()\n",
    "estadisticas['rango'] = estadisticas['max'] - estadisticas['min']\n",
    "estadisticas['IQR'] = df.quantile(0.75) - df.quantile(0.25)\n",
    "estadisticas['coef_var (%)'] = (estadisticas['std'] / estadisticas['mean']) * 100\n",
    "estadisticas['asimetria'] = df.skew()\n",
    "estadisticas['curtosis'] = df.kurtosis()\n",
    "\n",
    "# Distribución de calidad\n",
    "frecuencias = df['quality'].value_counts().sort_index()\n",
    "porcentajes = (frecuencias / len(df) * 100).round(2)\n",
    "acumulado = porcentajes.cumsum()\n",
    "\n",
    "df_quality = pd.DataFrame({\n",
    "    'Calidad': frecuencias.index,\n",
    "    'Frecuencia': frecuencias.values,\n",
    "    'Porcentaje (%)': porcentajes.values,\n",
    "    'Acumulado (%)': acumulado.values\n",
    "})\n",
    "\n",
    "# Crear categorías de calidad\n",
    "def categorizar_calidad(q):\n",
    "    if q <= 4: return 'Baja'\n",
    "    elif q <= 6: return 'Media'\n",
    "    else: return 'Alta'\n",
    "\n",
    "df['quality_category'] = df['quality'].apply(categorizar_calidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TESTS DE NORMALIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/16] Ejecutando tests de normalidad...\n"
     ]
    }
   ],
   "source": [
    "print(\"[4/16] Ejecutando tests de normalidad...\")\n",
    "resultados_normalidad = []\n",
    "\n",
    "for col in columnas_continuas:\n",
    "    muestra = df[col].dropna().sample(min(5000, len(df)), random_state=RANDOM_STATE)\n",
    "    stat_shapiro, p_shapiro = shapiro(muestra)\n",
    "    stat_dagostino, p_dagostino = normaltest(df[col].dropna())\n",
    "    es_normal = \"Sí\" if (p_shapiro > 0.05 and p_dagostino > 0.05) else \"No\"\n",
    "    \n",
    "    resultados_normalidad.append({\n",
    "        'Variable': col,\n",
    "        'Shapiro-Wilk (p)': p_shapiro,\n",
    "        \"D'Agostino (p)\": p_dagostino,\n",
    "        'Normal': es_normal\n",
    "    })\n",
    "\n",
    "df_normalidad = pd.DataFrame(resultados_normalidad)\n",
    "\n",
    "# Q-Q plots\n",
    "n_cols = 3\n",
    "n_rows = (len(columnas_continuas) + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(columnas_continuas):\n",
    "    stats.probplot(df[col].dropna(), dist=\"norm\", plot=axes[i])\n",
    "    axes[i].set_title(f'Q-Q Plot: {col}', fontsize=10, fontweight='bold')\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle('Q-Q Plots para Verificación de Normalidad', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/02_qq_plots_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. VISUALIZACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/16] Generando visualizaciones...\n"
     ]
    }
   ],
   "source": [
    "print(\"[5/16] Generando visualizaciones...\")\n",
    "\n",
    "# Histogramas con KDE\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(columnas_continuas):\n",
    "    sns.histplot(df[col].dropna(), bins=40, kde=True, ax=axes[i], color='steelblue', alpha=0.7)\n",
    "    axes[i].axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Media: {df[col].mean():.2f}')\n",
    "    axes[i].axvline(df[col].median(), color='green', linestyle=':', linewidth=2, label=f'Mediana: {df[col].median():.2f}')\n",
    "    axes[i].set_title(f'{col}', fontsize=11, fontweight='bold')\n",
    "    axes[i].legend(fontsize=8)\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "plt.suptitle('Distribución de Variables (Histogramas + KDE)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/03_histogramas_kde_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Boxplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "for i, col in enumerate(columnas_continuas):\n",
    "    bp = axes[i].boxplot(df[col].dropna(), patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('lightblue')\n",
    "    Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()\n",
    "    axes[i].set_title(f'{col}\\n({outliers} outliers)', fontsize=10, fontweight='bold')\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "plt.suptitle('Boxplots - Detección de Outliers (Método IQR)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/04_boxplots_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Violin plots por categoría\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "orden_categorias = ['Baja', 'Media', 'Alta']\n",
    "for i, col in enumerate(columnas_continuas):\n",
    "    sns.violinplot(data=df, x='quality_category', y=col, order=orden_categorias, palette='RdYlGn', ax=axes[i])\n",
    "    axes[i].set_title(f'{col}', fontsize=10, fontweight='bold')\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "plt.suptitle('Distribución por Categoría de Calidad', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/05_violin_plots_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Barras de quality\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colores = sns.color_palette('RdYlGn', n_colors=len(frecuencias))\n",
    "bars = axes[0].bar(frecuencias.index.astype(str), frecuencias.values, color=colores, edgecolor='black')\n",
    "for bar, freq, pct in zip(bars, frecuencias.values, porcentajes.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height(), f'{freq}\\n({pct}%)', ha='center', va='bottom', fontsize=9)\n",
    "axes[0].set_title('Distribución por Nivel de Calidad', fontsize=12, fontweight='bold')\n",
    "\n",
    "cat_counts = df['quality_category'].value_counts()[orden_categorias]\n",
    "colores_cat = ['#d73027', '#fee08b', '#1a9850']\n",
    "bars2 = axes[1].bar(cat_counts.index, cat_counts.values, color=colores_cat, edgecolor='black')\n",
    "for bar, freq in zip(bars2, cat_counts.values):\n",
    "    pct = freq / len(df) * 100\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height(), f'{freq}\\n({pct:.1f}%)', ha='center', va='bottom')\n",
    "axes[1].set_title('Distribución por Categoría', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/06_barras_quality_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ANÁLISIS DE OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/16] Analizando outliers...\n"
     ]
    }
   ],
   "source": [
    "print(\"[6/16] Analizando outliers...\")\n",
    "resultados_outliers = []\n",
    "for col in columnas_continuas:\n",
    "    Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers_iqr = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()\n",
    "    z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "    outliers_zscore = (z_scores > 3).sum()\n",
    "    \n",
    "    resultados_outliers.append({\n",
    "        'Variable': col,\n",
    "        'Q1': Q1, 'Q3': Q3, 'IQR': IQR,\n",
    "        'Límite Inferior': Q1 - 1.5*IQR,\n",
    "        'Límite Superior': Q3 + 1.5*IQR,\n",
    "        'Outliers (IQR)': outliers_iqr,\n",
    "        '% Outliers': (outliers_iqr / len(df)) * 100\n",
    "    })\n",
    "\n",
    "df_outliers = pd.DataFrame(resultados_outliers)\n",
    "total_outliers_iqr = df_outliers['Outliers (IQR)'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ANÁLISIS DE CORRELACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/16] Calculando correlaciones...\n"
     ]
    }
   ],
   "source": [
    "print(\"[7/16] Calculando correlaciones...\")\n",
    "corr_pearson = df[columnas_numericas].corr(method='pearson')\n",
    "corr_spearman = df[columnas_numericas].corr(method='spearman')\n",
    "\n",
    "# Heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "mask = np.triu(np.ones_like(corr_pearson, dtype=bool))\n",
    "sns.heatmap(corr_pearson, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', center=0, square=True, ax=axes[0])\n",
    "axes[0].set_title('Correlación de Pearson', fontsize=12, fontweight='bold')\n",
    "sns.heatmap(corr_spearman, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', center=0, square=True, ax=axes[1])\n",
    "axes[1].set_title('Correlación de Spearman', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/07_heatmaps_correlacion_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Top correlaciones\n",
    "def obtener_top_correlaciones(matriz_corr, n=10):\n",
    "    pares = []\n",
    "    for i in range(len(matriz_corr.columns)):\n",
    "        for j in range(i+1, len(matriz_corr.columns)):\n",
    "            col1, col2 = matriz_corr.columns[i], matriz_corr.columns[j]\n",
    "            corr = matriz_corr.iloc[i, j]\n",
    "            pares.append((col1, col2, corr, abs(corr)))\n",
    "    pares.sort(key=lambda x: x[3], reverse=True)\n",
    "    return pares[:n]\n",
    "\n",
    "top_corr = obtener_top_correlaciones(corr_pearson, 10)\n",
    "df_top_corr = pd.DataFrame(top_corr, columns=['Variable 1', 'Variable 2', 'Correlación', '|Correlación|'])\n",
    "df_top_corr.index = range(1, len(df_top_corr) + 1)\n",
    "\n",
    "# Correlaciones con quality\n",
    "corr_con_quality = corr_pearson['quality'].drop('quality').sort_values(key=abs, ascending=False)\n",
    "df_corr_quality = pd.DataFrame({\n",
    "    'Variable': corr_con_quality.index,\n",
    "    'Pearson': corr_con_quality.values,\n",
    "    'Spearman': corr_spearman['quality'].drop('quality')[corr_con_quality.index].values\n",
    "})\n",
    "df_corr_quality.index = range(1, len(df_corr_quality) + 1)\n",
    "\n",
    "# Gráfico correlaciones con quality\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['green' if c > 0 else 'red' for c in corr_con_quality.values]\n",
    "bars = ax.barh(corr_con_quality.index, corr_con_quality.values, color=colors, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.set_title('Correlación de Variables con Calidad del Vino', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, corr_con_quality.values):\n",
    "    ax.text(val + 0.02 if val > 0 else val - 0.02, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/08_correlacion_con_quality_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TESTS ESTADÍSTICOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/16] Ejecutando tests estadísticos...\n"
     ]
    }
   ],
   "source": [
    "print(\"[8/16] Ejecutando tests estadísticos...\")\n",
    "resultados_tests = []\n",
    "for col in columnas_continuas:\n",
    "    grupos = [df[df['quality'] == q][col].dropna() for q in sorted(df['quality'].unique())]\n",
    "    stat_anova, p_anova = f_oneway(*grupos)\n",
    "    stat_kruskal, p_kruskal = kruskal(*grupos)\n",
    "    \n",
    "    resultados_tests.append({\n",
    "        'Variable': col,\n",
    "        'ANOVA (F)': stat_anova,\n",
    "        'ANOVA (p)': p_anova,\n",
    "        'Kruskal (H)': stat_kruskal,\n",
    "        'Kruskal (p)': p_kruskal,\n",
    "        'Significativo': \"Sí\" if p_kruskal < 0.05 else \"No\"\n",
    "    })\n",
    "\n",
    "df_tests = pd.DataFrame(resultados_tests)\n",
    "vars_significativas = df_tests[df_tests['Significativo'] == 'Sí']['Variable'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ESTADÍSTICAS POR GRUPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/16] Calculando estadísticas por grupo...\n"
     ]
    }
   ],
   "source": [
    "print(\"[9/16] Calculando estadísticas por grupo...\")\n",
    "stats_por_calidad = df.groupby('quality')[columnas_continuas].agg(['mean', 'std', 'median'])\n",
    "\n",
    "df_baja = df[df['quality'] <= 4]\n",
    "df_alta = df[df['quality'] >= 7]\n",
    "comparacion = pd.DataFrame({\n",
    "    'Variable': columnas_continuas,\n",
    "    'Media (Baja)': df_baja[columnas_continuas].mean().values,\n",
    "    'Media (Alta)': df_alta[columnas_continuas].mean().values,\n",
    "    'Diferencia': (df_alta[columnas_continuas].mean() - df_baja[columnas_continuas].mean()).values,\n",
    "    'Diferencia (%)': ((df_alta[columnas_continuas].mean() - df_baja[columnas_continuas].mean()) / df_baja[columnas_continuas].mean() * 100).values\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/16] Ejecutando PCA...\n"
     ]
    }
   ],
   "source": [
    "print(\"[10/16] Ejecutando PCA...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[columnas_continuas])\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "varianza_explicada = pca.explained_variance_ratio_\n",
    "varianza_acumulada = np.cumsum(varianza_explicada)\n",
    "\n",
    "n_comp_80 = np.argmax(varianza_acumulada >= 0.80) + 1\n",
    "n_comp_95 = np.argmax(varianza_acumulada >= 0.95) + 1\n",
    "\n",
    "# Gráficos PCA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].bar(range(1, len(varianza_explicada)+1), varianza_explicada * 100, alpha=0.7, color='steelblue')\n",
    "axes[0].plot(range(1, len(varianza_explicada)+1), varianza_explicada * 100, 'ro-')\n",
    "axes[0].set_title('Scree Plot', fontsize=12, fontweight='bold')\n",
    "axes[1].plot(range(1, len(varianza_acumulada)+1), varianza_acumulada * 100, 'bo-')\n",
    "axes[1].axhline(y=80, color='red', linestyle='--', label='80%')\n",
    "axes[1].axhline(y=95, color='green', linestyle='--', label='95%')\n",
    "axes[1].set_title('Varianza Acumulada', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/09_pca_varianza_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# PCA scatter\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=df['quality'], cmap='RdYlGn', alpha=0.6, s=30)\n",
    "ax.set_xlabel(f'PC1 ({varianza_explicada[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({varianza_explicada[1]*100:.1f}%)')\n",
    "ax.set_title('Proyección PCA', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Calidad')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/10_pca_scatter_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Loadings\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(len(columnas_continuas))], index=columnas_continuas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/16] Ejecutando clustering...\n"
     ]
    }
   ],
   "source": [
    "print(\"[11/16] Ejecutando clustering...\")\n",
    "inertias, silhouettes = [], []\n",
    "K_range = range(2, 11)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "mejor_k = K_range[np.argmax(silhouettes)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(K_range, inertias, 'bo-')\n",
    "axes[0].set_title('Método del Codo', fontweight='bold')\n",
    "axes[1].plot(K_range, silhouettes, 'go-')\n",
    "axes[1].set_title('Silhouette Score', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/11_clustering_elbow_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "n_clusters = 4\n",
    "kmeans_final = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10)\n",
    "df['cluster'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=df['cluster'], cmap='viridis', alpha=0.6, s=30)\n",
    "centroides_pca = pca.transform(kmeans_final.cluster_centers_)\n",
    "ax.scatter(centroides_pca[:, 0], centroides_pca[:, 1], c='red', marker='X', s=200, edgecolors='black')\n",
    "ax.set_title(f'Clusters K-Means (K={n_clusters})', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/12_clustering_pca_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "perfil_clusters = df.groupby('cluster')[columnas_continuas + ['quality']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. MACHINE LEARNING - CLASIFICACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/16] Entrenando modelos de clasificación...\n"
     ]
    }
   ],
   "source": [
    "print(\"[12/16] Entrenando modelos de clasificación...\")\n",
    "X = df[columnas_continuas]\n",
    "y = df['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "scaler_ml = StandardScaler()\n",
    "X_train_scaled = scaler_ml.fit_transform(X_train)\n",
    "X_test_scaled = scaler_ml.transform(X_test)\n",
    "\n",
    "modelos = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "resultados_modelos = []\n",
    "for nombre, modelo in modelos.items():\n",
    "    modelo.fit(X_train_scaled, y_train)\n",
    "    y_pred = modelo.predict(X_test_scaled)\n",
    "    cv_scores = cross_val_score(modelo, X_train_scaled, y_train, cv=5)\n",
    "    \n",
    "    resultados_modelos.append({\n",
    "        'Modelo': nombre,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'F1-Score': f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std()\n",
    "    })\n",
    "\n",
    "df_resultados_ml = pd.DataFrame(resultados_modelos).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Matriz de confusión (mejor modelo)\n",
    "mejor_modelo = modelos['Random Forest']\n",
    "y_pred_best = mejor_modelo.predict(X_test_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))\n",
    "ax.set_title('Matriz de Confusión - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/13_confusion_matrix_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Comparación de modelos\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x_pos = np.arange(len(df_resultados_ml))\n",
    "width = 0.2\n",
    "ax.bar(x_pos - width*1.5, df_resultados_ml['Accuracy'], width, label='Accuracy', color='steelblue')\n",
    "ax.bar(x_pos - width/2, df_resultados_ml['Precision'], width, label='Precision', color='green')\n",
    "ax.bar(x_pos + width/2, df_resultados_ml['Recall'], width, label='Recall', color='orange')\n",
    "ax.bar(x_pos + width*1.5, df_resultados_ml['F1-Score'], width, label='F1-Score', color='red')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(df_resultados_ml['Modelo'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Comparación de Modelos', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/14_comparacion_modelos_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/16] Calculando feature importance...\n"
     ]
    }
   ],
   "source": [
    "print(\"[13/16] Calculando feature importance...\")\n",
    "importancias = mejor_modelo.feature_importances_\n",
    "indices = np.argsort(importancias)[::-1]\n",
    "df_importancia = pd.DataFrame({\n",
    "    'Variable': [columnas_continuas[i] for i in indices],\n",
    "    'Importancia': [importancias[i] for i in indices],\n",
    "    'Importancia (%)': [importancias[i] * 100 for i in indices]\n",
    "})\n",
    "df_importancia.index = range(1, len(df_importancia) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(df_importancia)))\n",
    "bars = ax.barh(df_importancia['Variable'], df_importancia['Importancia'], color=colors, edgecolor='black')\n",
    "ax.set_title('Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, df_importancia['Importancia']):\n",
    "    ax.text(val + 0.005, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=9)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/15_feature_importance_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. REGRESIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/16] Entrenando modelos de regresión...\n"
     ]
    }
   ],
   "source": [
    "print(\"[14/16] Entrenando modelos de regresión...\")\n",
    "reg_lr = LinearRegression()\n",
    "reg_rf = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE)\n",
    "\n",
    "reg_lr.fit(X_train_scaled, y_train)\n",
    "reg_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_lr = reg_lr.predict(X_test_scaled)\n",
    "y_pred_rf = reg_rf.predict(X_test_scaled)\n",
    "\n",
    "resultados_reg = []\n",
    "for nombre, y_pred_reg in [('Linear Regression', y_pred_lr), ('Random Forest Regressor', y_pred_rf)]:\n",
    "    resultados_reg.append({\n",
    "        'Modelo': nombre,\n",
    "        'MSE': mean_squared_error(y_test, y_pred_reg),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred_reg)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred_reg),\n",
    "        'R²': r2_score(y_test, y_pred_reg)\n",
    "    })\n",
    "df_resultados_reg = pd.DataFrame(resultados_reg)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].scatter(y_test, y_pred_lr, alpha=0.5, color='blue')\n",
    "axes[0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "axes[0].set_title(f'Linear Regression (R² = {r2_score(y_test, y_pred_lr):.3f})', fontweight='bold')\n",
    "axes[1].scatter(y_test, y_pred_rf, alpha=0.5, color='green')\n",
    "axes[1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "axes[1].set_title(f'Random Forest (R² = {r2_score(y_test, y_pred_rf):.3f})', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/16_regression_predictions_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "coef_lr = pd.DataFrame({\n",
    "    'Variable': columnas_continuas,\n",
    "    'Coeficiente': reg_lr.coef_\n",
    "}).sort_values('Coeficiente', key=abs, ascending=False)\n",
    "coef_lr.index = range(1, len(coef_lr) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. RESUMEN PARA CLAUDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/16] Generando resumen...\n",
      "   Resumen guardado en: reports/resumen_analisis_v2.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"[15/16] Generando resumen...\")\n",
    "\n",
    "resumen_completo = f\"\"\"\n",
    "================================================================================\n",
    "RESUMEN COMPLETO DEL ANÁLISIS DE DATOS - WINE QUALITY (WHITE WINE)\n",
    "================================================================================\n",
    "\n",
    "1. INFORMACIÓN DEL DATASET\n",
    "--------------------------------------------------------------------------------\n",
    "   - Nombre: Wine Quality - White Wine\n",
    "   - Fuente: UCI Machine Learning Repository\n",
    "   - Dimensiones: {df.shape[0]:,} registros x {df.shape[1]} columnas\n",
    "   - Variables continuas: {len(columnas_continuas)}\n",
    "   - Variable objetivo: quality (valores {df['quality'].min()} a {df['quality'].max()})\n",
    "   - Registros duplicados: {info_dataset['Duplicados']:,}\n",
    "\n",
    "2. CALIDAD DE LOS DATOS\n",
    "--------------------------------------------------------------------------------\n",
    "   - Valores faltantes: {df.isnull().sum().sum()} ({porcentaje_faltantes.mean():.4f}%)\n",
    "   - Total de outliers (IQR): {total_outliers_iqr:,}\n",
    "   - Variables con más outliers: {', '.join(df_outliers.nlargest(3, 'Outliers (IQR)')['Variable'].tolist())}\n",
    "\n",
    "3. DISTRIBUCIÓN DE LA VARIABLE OBJETIVO\n",
    "--------------------------------------------------------------------------------\n",
    "{df_quality.to_string()}\n",
    "\n",
    "   Categorías:\n",
    "   - Baja (3-4): {(df['quality_category'] == 'Baja').sum()} ({(df['quality_category'] == 'Baja').sum()/len(df)*100:.2f}%)\n",
    "   - Media (5-6): {(df['quality_category'] == 'Media').sum()} ({(df['quality_category'] == 'Media').sum()/len(df)*100:.2f}%)\n",
    "   - Alta (7-9): {(df['quality_category'] == 'Alta').sum()} ({(df['quality_category'] == 'Alta').sum()/len(df)*100:.2f}%)\n",
    "\n",
    "4. ESTADÍSTICAS DESCRIPTIVAS\n",
    "--------------------------------------------------------------------------------\n",
    "{estadisticas[['mean', 'std', 'min', 'max', 'mediana', 'asimetria', 'curtosis']].to_string()}\n",
    "\n",
    "5. TESTS DE NORMALIDAD\n",
    "--------------------------------------------------------------------------------\n",
    "{df_normalidad.to_string()}\n",
    "\n",
    "6. ANÁLISIS DE CORRELACIONES\n",
    "--------------------------------------------------------------------------------\n",
    "   TOP 10 CORRELACIONES MÁS FUERTES:\n",
    "{df_top_corr.to_string()}\n",
    "\n",
    "   CORRELACIONES CON CALIDAD:\n",
    "{df_corr_quality.to_string()}\n",
    "\n",
    "7. TESTS ESTADÍSTICOS (DIFERENCIAS ENTRE GRUPOS DE CALIDAD)\n",
    "--------------------------------------------------------------------------------\n",
    "{df_tests.to_string()}\n",
    "\n",
    "   Variables con diferencias significativas (Kruskal-Wallis, p<0.05):\n",
    "   {', '.join(vars_significativas)}\n",
    "\n",
    "8. ANÁLISIS DE COMPONENTES PRINCIPALES (PCA)\n",
    "--------------------------------------------------------------------------------\n",
    "   - Componentes para 80% de varianza: {n_comp_80}\n",
    "   - Componentes para 95% de varianza: {n_comp_95}\n",
    "   - Varianza del primer componente: {varianza_explicada[0]*100:.2f}%\n",
    "   - Varianza de los primeros 3 componentes: {varianza_acumulada[2]*100:.2f}%\n",
    "   \n",
    "   Loadings PC1:\n",
    "{loadings['PC1'].sort_values(key=abs, ascending=False).to_string()}\n",
    "\n",
    "9. CLUSTERING\n",
    "--------------------------------------------------------------------------------\n",
    "   - Número de clusters óptimo (Silhouette): {mejor_k}\n",
    "   - Clusters utilizados: {n_clusters}\n",
    "   - Distribución: {dict(df['cluster'].value_counts().sort_index())}\n",
    "   \n",
    "   Perfil de clusters (medias):\n",
    "{perfil_clusters.round(3).to_string()}\n",
    "\n",
    "10. RESULTADOS DE MACHINE LEARNING - CLASIFICACIÓN\n",
    "--------------------------------------------------------------------------------\n",
    "{df_resultados_ml.to_string()}\n",
    "\n",
    "    Mejor modelo: {df_resultados_ml.iloc[0]['Modelo']}\n",
    "    - Accuracy: {df_resultados_ml.iloc[0]['Accuracy']:.4f}\n",
    "    - F1-Score: {df_resultados_ml.iloc[0]['F1-Score']:.4f}\n",
    "    - Cross-Validation: {df_resultados_ml.iloc[0]['CV Mean']:.4f} (+/- {df_resultados_ml.iloc[0]['CV Std']:.4f})\n",
    "\n",
    "11. FEATURE IMPORTANCE (RANDOM FOREST)\n",
    "--------------------------------------------------------------------------------\n",
    "{df_importancia.to_string()}\n",
    "\n",
    "12. RESULTADOS DE REGRESIÓN\n",
    "--------------------------------------------------------------------------------\n",
    "{df_resultados_reg.to_string()}\n",
    "\n",
    "   Coeficientes de Regresión Lineal (intercepto: {reg_lr.intercept_:.4f}):\n",
    "{coef_lr.to_string()}\n",
    "\n",
    "13. COMPARACIÓN CALIDAD BAJA vs ALTA\n",
    "--------------------------------------------------------------------------------\n",
    "{comparacion.round(4).to_string()}\n",
    "\n",
    "14. ANÁLISIS DE OUTLIERS DETALLADO\n",
    "--------------------------------------------------------------------------------\n",
    "{df_outliers.to_string()}\n",
    "\n",
    "15. OBSERVACIONES CLAVE\n",
    "--------------------------------------------------------------------------------\n",
    "   - El alcohol es la variable más importante para predecir calidad ({df_importancia.iloc[0]['Importancia']*100:.1f}% de importancia)\n",
    "   - Existe una fuerte correlación negativa entre densidad y alcohol ({corr_pearson.loc['density', 'alcohol']:.3f})\n",
    "   - La mayoría de vinos son de calidad media (5-6), representando {(df['quality_category'] == 'Media').sum()/len(df)*100:.1f}% del dataset\n",
    "   - {len(vars_significativas)} de {len(columnas_continuas)} variables muestran diferencias significativas entre grupos de calidad\n",
    "   - El modelo Random Forest logra un accuracy de {df_resultados_ml.iloc[0]['Accuracy']*100:.1f}%\n",
    "   - Ninguna variable sigue una distribución normal (todas tienen p < 0.05 en tests de normalidad)\n",
    "   - Los vinos de alta calidad tienen en promedio {comparacion[comparacion['Variable']=='alcohol']['Diferencia (%)'].values[0]:.1f}% más alcohol que los de baja calidad\n",
    "\"\"\"\n",
    "\n",
    "# Guardar resumen\n",
    "with open('reports/resumen_analisis_v2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(resumen_completo)\n",
    "\n",
    "print(\"   Resumen guardado en: reports/resumen_analisis_v2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. GENERAR INSIGHTS CON CLAUDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/16] Generando insights con Claude...\n",
      "   Insights guardados en: reports/insights_claude_v2.txt\n",
      "\n",
      "======================================================================\n",
      "REPORTE EJECUTIVO GENERADO\n",
      "======================================================================\n",
      "# REPORTE EJECUTIVO: ANÁLISIS DE CALIDAD DE VINOS BLANCOS\n",
      "\n",
      "## 1. RESUMEN EJECUTIVO\n",
      "\n",
      "El presente análisis examina un dataset de 4,898 vinos blancos proveniente del UCI Machine Learning Repository, con 11 variables fisicoquímicas y una variable objetivo de calidad (escala 3-9). Los datos presentan una calidad excepcional con ausencia total de valores faltantes, aunque se identificaron 937 registros duplicados y 1,063 outliers distribuidos principalmente en las variables ácido cítrico, cloruros y acidez volátil.\n",
      "\n",
      "La distribución de la variable objetivo revela un fuerte desbalance de clases, con el 74.62% de los vinos concentrados en las categorías de calidad media (5-6), mientras que solo el 3.74% corresponde a vinos de baja calidad (3-4) y el 21.64% a alta calidad (7-9). Esta distribución asimétrica presenta desafíos significativos para el desarrollo de modelos predictivos balanceados.\n",
      "\n",
      "Los hallazgos estadísticos revelan que el contenido alcohólico emerge como el predictor más importante de calidad, mostrando una correlación positiva de 0.436 y representando el 11.7% de la importancia en modelos de ensemble. Todas las 11 variables fisicoquímicas demuestran diferencias estadísticamente significativas entre grupos de calidad (p<0.05), indicando su relevancia predictiva. El análisis de clustering identifica cuatro perfiles distintos de vinos, caracterizados principalmente por diferencias en contenido alcohólico, azúcar residual y densidad.\n",
      "\n",
      "Los modelos de machine learning alcanzan un rendimiento moderado, con Random Forest obteniendo la mejor performance (67.55% de accuracy), aunque limitado por el desbalance de clases y la complejidad inherente de la evaluación sensorial. El análisis de componentes principales revela que se requieren 6 componentes para explicar el 80% de la varianza, sugiriendo una estructura de datos relativamente compleja con múltiples dimensiones de variabilidad.\n",
      "\n",
      "## 2. ANÁLISIS DE LA CALIDAD DE DATOS\n",
      "\n",
      "**Evaluación de valores faltantes:** El dataset ...\n",
      "[Ver archivo completo en reports/insights_claude_v2.txt]\n"
     ]
    }
   ],
   "source": [
    "print(\"[16/16] Generando insights con Claude...\")\n",
    "\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    try:\n",
    "        from anthropic import Anthropic\n",
    "        \n",
    "        client = Anthropic(api_key=api_key)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Eres un experto en análisis de datos, ciencia de datos y machine learning. \n",
    "Analiza el siguiente resumen completo de un análisis de datos sobre calidad de vinos blancos.\n",
    "\n",
    "Genera un REPORTE EJECUTIVO PROFESIONAL Y DETALLADO que incluya:\n",
    "\n",
    "1. RESUMEN EJECUTIVO (3-4 párrafos)\n",
    "   - Descripción del dataset y contexto del problema\n",
    "   - Calidad general de los datos\n",
    "   - Principales descubrimientos\n",
    "\n",
    "2. ANÁLISIS DE LA CALIDAD DE DATOS\n",
    "   - Evaluación de valores faltantes\n",
    "   - Análisis de outliers y su impacto\n",
    "   - Distribución de la variable objetivo y sus implicaciones\n",
    "\n",
    "3. HALLAZGOS ESTADÍSTICOS CLAVE (5 hallazgos)\n",
    "   - Patrones y tendencias descubiertos\n",
    "   - Relaciones significativas entre variables\n",
    "   - Interpretación de los tests estadísticos\n",
    "\n",
    "4. ANÁLISIS DE MACHINE LEARNING\n",
    "   - Evaluación del rendimiento de los modelos\n",
    "   - Interpretación del feature importance\n",
    "   - Capacidad predictiva y limitaciones\n",
    "\n",
    "5. ANÁLISIS DE CLUSTERING Y PCA\n",
    "   - Interpretación de los clusters encontrados\n",
    "   - Significado de los componentes principales\n",
    "   - Patrones de agrupamiento de vinos\n",
    "\n",
    "6. RECOMENDACIONES DE PREPROCESAMIENTO (5 recomendaciones)\n",
    "   - Técnicas para mejorar la calidad de datos\n",
    "   - Estrategias para manejar outliers\n",
    "   - Transformaciones sugeridas\n",
    "   - Estrategias para el desbalance de clases\n",
    "\n",
    "7. LIMITACIONES DEL ANÁLISIS\n",
    "   - Restricciones del dataset\n",
    "   - Consideraciones metodológicas\n",
    "   - Posibles sesgos\n",
    "\n",
    "8. CONCLUSIONES Y PRÓXIMOS PASOS\n",
    "   - Síntesis de hallazgos principales\n",
    "   - Recomendaciones para investigación futura\n",
    "   - Aplicaciones prácticas\n",
    "\n",
    "DATOS DEL ANÁLISIS:\n",
    "{resumen_completo}\n",
    "\n",
    "Responde en español, de manera profesional y detallada. Usa datos específicos del análisis para respaldar cada punto.\n",
    "\"\"\"\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-sonnet-4-20250514\",\n",
    "            max_tokens=8000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        insights = response.content[0].text\n",
    "        \n",
    "        with open('reports/insights_claude_v2.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(\"REPORTE EJECUTIVO - ANÁLISIS DE CALIDAD DE VINOS BLANCOS\\n\")\n",
    "            f.write(\"=\"*70 + \"\\n\\n\")\n",
    "            f.write(insights)\n",
    "        \n",
    "        print(\"   Insights guardados en: reports/insights_claude_v2.txt\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"REPORTE EJECUTIVO GENERADO\")\n",
    "        print(\"=\"*70)\n",
    "        print(insights[:2000] + \"...\\n[Ver archivo completo en reports/insights_claude_v2.txt]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error al generar insights: {e}\")\n",
    "else:\n",
    "    print(\"   API key no encontrada. Configura ANTHROPIC_API_KEY en .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINALIZACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ANÁLISIS COMPLETADO\n",
      "======================================================================\n",
      "\n",
      "ARCHIVOS GENERADOS:\n",
      "\n",
      "  Visualizaciones (outputs/):\n",
      "    - 01_heatmap_valores_faltantes_v2.png\n",
      "    - 02_qq_plots_v2.png\n",
      "    - 03_histogramas_kde_v2.png\n",
      "    - 04_boxplots_v2.png\n",
      "    - 05_violin_plots_v2.png\n",
      "    - 06_barras_quality_v2.png\n",
      "    - 07_heatmaps_correlacion_v2.png\n",
      "    - 08_correlacion_con_quality_v2.png\n",
      "    - 09_pca_varianza_v2.png\n",
      "    - 10_pca_scatter_v2.png\n",
      "    - 11_clustering_elbow_v2.png\n",
      "    - 12_clustering_pca_v2.png\n",
      "    - 13_confusion_matrix_v2.png\n",
      "    - 14_comparacion_modelos_v2.png\n",
      "    - 15_feature_importance_v2.png\n",
      "    - 16_regression_predictions_v2.png\n",
      "\n",
      "  Reportes (reports/):\n",
      "    - resumen_analisis_v2.txt\n",
      "    - insights_claude_v2.txt\n",
      "\n",
      "MÉTRICAS CLAVE:\n",
      "  - Dataset: 4,898 registros x 14 columnas\n",
      "  - Outliers totales: 1,063\n",
      "  - Mejor modelo: Random Forest (Accuracy: 67.6%)\n",
      "  - Variable más importante: alcohol (11.7%)\n",
      "  - Correlación más fuerte: residual sugar vs density (0.839)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANÁLISIS COMPLETADO\")\n",
    "print(\"=\"*70)\n",
    "print(\n",
    "    f\"\"\"\n",
    "ARCHIVOS GENERADOS:\n",
    "  \n",
    "  Visualizaciones (outputs/):\n",
    "    - 01_heatmap_valores_faltantes_v2.png\n",
    "    - 02_qq_plots_v2.png\n",
    "    - 03_histogramas_kde_v2.png\n",
    "    - 04_boxplots_v2.png\n",
    "    - 05_violin_plots_v2.png\n",
    "    - 06_barras_quality_v2.png\n",
    "    - 07_heatmaps_correlacion_v2.png\n",
    "    - 08_correlacion_con_quality_v2.png\n",
    "    - 09_pca_varianza_v2.png\n",
    "    - 10_pca_scatter_v2.png\n",
    "    - 11_clustering_elbow_v2.png\n",
    "    - 12_clustering_pca_v2.png\n",
    "    - 13_confusion_matrix_v2.png\n",
    "    - 14_comparacion_modelos_v2.png\n",
    "    - 15_feature_importance_v2.png\n",
    "    - 16_regression_predictions_v2.png\n",
    "    \n",
    "  Reportes (reports/):\n",
    "    - resumen_analisis_v2.txt\n",
    "    - insights_claude_v2.txt\n",
    "\n",
    "MÉTRICAS CLAVE:\n",
    "  - Dataset: {df.shape[0]:,} registros x {df.shape[1]} columnas\n",
    "  - Outliers totales: {total_outliers_iqr:,}\n",
    "  - Mejor modelo: {df_resultados_ml.iloc[0]['Modelo']} (Accuracy: {df_resultados_ml.iloc[0]['Accuracy']*100:.1f}%)\n",
    "  - Variable más importante: {df_importancia.iloc[0]['Variable']} ({df_importancia.iloc[0]['Importancia']*100:.1f}%)\n",
    "  - Correlación más fuerte: {top_corr[0][0]} vs {top_corr[0][1]} ({top_corr[0][2]:.3f})\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
