{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä An√°lisis Automatizado Avanzado de Datos con IA\n",
    "## Versi√≥n 2: An√°lisis Completo + ML + PCA + Clustering + Claude\n",
    "\n",
    "**An√°lisis exhaustivo autom√°tico de cualquier CSV**\n",
    "\n",
    "### Requisitos: M√≠nimo 2,000 registros y 10 columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öôÔ∏è CONFIGURACI√ìN - MODIFICA SOLO ESTA CELDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üîß CONFIGURACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "CSV_PATH = \"data/tu_archivo.csv\"  # Ruta al CSV\n",
    "\n",
    "DATASET_DESCRIPTION = \"\"\"\n",
    "Describe tu dataset aqu√≠ (2-3 oraciones).\n",
    "Ejemplo: Dataset de ventas con informaci√≥n de productos y clientes.\n",
    "\"\"\"\n",
    "\n",
    "DATASET_NAME = \"Mi Dataset\"\n",
    "TARGET_VARIABLE = None  # Variable objetivo (None = auto-detectar)\n",
    "PROBLEM_TYPE = None  # 'clasificacion', 'regresion', o None (auto)\n",
    "CSV_SEPARATOR = \",\"\n",
    "N_CLUSTERS = None  # N√∫mero de clusters (None = autom√°tico)\n",
    "\n",
    "print(f\"Configuraci√≥n: {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1- Importar Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, normaltest, f_oneway, kruskal\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings, os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "load_dotenv()\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "print('Librer√≠as cargadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2- Cargar y Validar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH, sep=CSV_SEPARATOR)\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df.columns = df.columns.str.strip()\n",
    "print(f'Cargado: {df.shape[0]:,} x {df.shape[1]}')\n",
    "print(f\"{'‚úÖ' if df.shape[0]>=2000 else '‚ùå'} Registros: {df.shape[0]:,}\")\n",
    "print(f\"{'‚úÖ' if df.shape[1]>=10 else '‚ùå'} Columnas: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar tipos de variables autom√°ticamente\n",
    "columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "columnas_objeto = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "columnas_categoricas = columnas_objeto.copy()\n",
    "for col in columnas_numericas:\n",
    "    if df[col].nunique() <= 15:\n",
    "        columnas_categoricas.append(col)\n",
    "columnas_continuas = [c for c in columnas_numericas if c not in columnas_categoricas]\n",
    "\n",
    "# Detectar variable objetivo\n",
    "if TARGET_VARIABLE and TARGET_VARIABLE in df.columns:\n",
    "    variable_objetivo = TARGET_VARIABLE\n",
    "elif columnas_categoricas:\n",
    "    variable_objetivo = columnas_categoricas[-1]\n",
    "else:\n",
    "    variable_objetivo = columnas_numericas[-1] if columnas_numericas else None\n",
    "\n",
    "# Tipo de problema\n",
    "if PROBLEM_TYPE:\n",
    "    tipo_problema = PROBLEM_TYPE\n",
    "elif variable_objetivo and (variable_objetivo in columnas_categoricas or df[variable_objetivo].nunique() <= 15):\n",
    "    tipo_problema = 'clasificacion'\n",
    "else:\n",
    "    tipo_problema = 'regresion'\n",
    "\n",
    "print(f'\\nContinuas: {len(columnas_continuas)} | Categ√≥ricas: {len(columnas_categoricas)}')\n",
    "print(f'Objetivo: {variable_objetivo} | Tipo: {tipo_problema}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3- Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_faltantes = df.isnull().sum()\n",
    "porcentaje_faltantes = (df.isnull().sum() / len(df)) * 100\n",
    "print(f'Total faltantes: {df.isnull().sum().sum():,} ({porcentaje_faltantes.mean():.2f}%)')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "plt.title(f'Valores Faltantes - {DATASET_NAME}', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/01_heatmap_faltantes.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4- Estad√≠sticas Descriptivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if columnas_continuas:\n",
    "    estadisticas = df[columnas_continuas].describe().T\n",
    "    estadisticas['mediana'] = df[columnas_continuas].median()\n",
    "    estadisticas['asimetria'] = df[columnas_continuas].skew()\n",
    "    estadisticas['curtosis'] = df[columnas_continuas].kurtosis()\n",
    "    display(estadisticas.round(4))\n",
    "else:\n",
    "    estadisticas = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5- Tests de Normalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_normalidad = []\n",
    "for col in columnas_continuas:\n",
    "    muestra = df[col].dropna().sample(min(5000, len(df)), random_state=42)\n",
    "    try:\n",
    "        _, p_shap = shapiro(muestra)\n",
    "        _, p_dag = normaltest(muestra)\n",
    "        normal = 'S√≠' if p_shap > 0.05 and p_dag > 0.05 else 'No'\n",
    "    except:\n",
    "        p_shap, p_dag, normal = None, None, 'Error'\n",
    "    resultados_normalidad.append({'Variable': col, 'Shapiro (p)': p_shap, \"D'Agostino (p)\": p_dag, 'Normal': normal})\n",
    "df_normalidad = pd.DataFrame(resultados_normalidad)\n",
    "display(df_normalidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6- Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas\n",
    "if columnas_continuas:\n",
    "    n = min(len(columnas_continuas), 12)\n",
    "    nc = 3\n",
    "    nr = (n + nc - 1) // nc\n",
    "    fig, axes = plt.subplots(nr, nc, figsize=(15, 4*nr))\n",
    "    axes = axes.flatten() if nr > 1 else [axes]\n",
    "    for i, col in enumerate(columnas_continuas[:n]):\n",
    "        sns.histplot(df[col].dropna(), bins=40, kde=True, ax=axes[i])\n",
    "        axes[i].axvline(df[col].mean(), color='red', linestyle='--')\n",
    "        axes[i].set_title(col, fontweight='bold')\n",
    "    for j in range(i+1, len(axes)): axes[j].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/02_histogramas.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "if columnas_continuas:\n",
    "    fig, axes = plt.subplots(nr, nc, figsize=(15, 4*nr))\n",
    "    axes = axes.flatten() if nr > 1 else [axes]\n",
    "    for i, col in enumerate(columnas_continuas[:n]):\n",
    "        bp = axes[i].boxplot(df[col].dropna(), patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
    "        outliers = ((df[col] < Q1 - 1.5*(Q3-Q1)) | (df[col] > Q3 + 1.5*(Q3-Q1))).sum()\n",
    "        axes[i].set_title(f'{col} ({outliers} outliers)', fontweight='bold')\n",
    "    for j in range(i+1, len(axes)): axes[j].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/03_boxplots.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7- Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_outliers = []\n",
    "total_outliers = 0\n",
    "for col in columnas_continuas:\n",
    "    Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    out = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()\n",
    "    total_outliers += out\n",
    "    resultados_outliers.append({'Variable': col, 'Outliers': out, '%': out/len(df)*100})\n",
    "df_outliers = pd.DataFrame(resultados_outliers).sort_values('Outliers', ascending=False)\n",
    "print(f'Total outliers: {total_outliers:,}')\n",
    "display(df_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8- Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(columnas_numericas) > 1:\n",
    "    corr_pearson = df[columnas_numericas].corr()\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_pearson, dtype=bool))\n",
    "    sns.heatmap(corr_pearson, mask=mask, annot=len(columnas_numericas)<=12, fmt='.2f', cmap='RdBu_r', center=0)\n",
    "    plt.title('Matriz de Correlaciones', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/04_correlaciones.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Top correlaciones\n",
    "    pares = []\n",
    "    for i in range(len(corr_pearson.columns)):\n",
    "        for j in range(i+1, len(corr_pearson.columns)):\n",
    "            pares.append((corr_pearson.columns[i], corr_pearson.columns[j], corr_pearson.iloc[i,j]))\n",
    "    pares.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    df_top_corr = pd.DataFrame(pares[:10], columns=['Var1', 'Var2', 'Corr'])\n",
    "    display(df_top_corr)\n",
    "else:\n",
    "    corr_pearson = pd.DataFrame()\n",
    "    df_top_corr = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9- Tests Estad√≠sticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_tests = []\n",
    "if variable_objetivo and tipo_problema == 'clasificacion':\n",
    "    for col in columnas_continuas:\n",
    "        grupos = [df[df[variable_objetivo]==g][col].dropna() for g in df[variable_objetivo].unique()]\n",
    "        grupos = [g for g in grupos if len(g) > 0]\n",
    "        if len(grupos) >= 2:\n",
    "            try:\n",
    "                _, p_anova = f_oneway(*grupos)\n",
    "                _, p_krus = kruskal(*grupos)\n",
    "                sig = 'S√≠' if p_krus < 0.05 else 'No'\n",
    "            except:\n",
    "                p_anova, p_krus, sig = None, None, 'Error'\n",
    "            resultados_tests.append({'Variable': col, 'ANOVA (p)': p_anova, 'Kruskal (p)': p_krus, 'Sig': sig})\n",
    "df_tests = pd.DataFrame(resultados_tests)\n",
    "if len(df_tests) > 0:\n",
    "    vars_sig = df_tests[df_tests['Sig']=='S√≠']['Variable'].tolist()\n",
    "    print(f'Variables significativas: {len(vars_sig)} de {len(columnas_continuas)}')\n",
    "    display(df_tests)\n",
    "else:\n",
    "    vars_sig = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10- PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(columnas_continuas) >= 2:\n",
    "    df_pca = df[columnas_continuas].dropna()\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df_pca)\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    var_exp = pca.explained_variance_ratio_\n",
    "    var_acum = np.cumsum(var_exp)\n",
    "    n_80 = np.argmax(var_acum >= 0.80) + 1\n",
    "    n_95 = np.argmax(var_acum >= 0.95) + 1\n",
    "    print(f'PC para 80%: {n_80} | PC para 95%: {n_95}')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    axes[0].bar(range(1, len(var_exp)+1), var_exp*100)\n",
    "    axes[0].set_title('Scree Plot', fontweight='bold')\n",
    "    axes[1].plot(range(1, len(var_acum)+1), var_acum*100, 'bo-')\n",
    "    axes[1].axhline(80, color='r', linestyle='--')\n",
    "    axes[1].set_title('Varianza Acumulada', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/05_pca.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    X_pca, var_exp, var_acum, n_80, n_95 = None, [], [], 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_pca is not None and len(X_scaled) > 100:\n",
    "    sils = []\n",
    "    for k in range(2, min(11, len(X_scaled)//10)):\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        km.fit(X_scaled)\n",
    "        sils.append(silhouette_score(X_scaled, km.labels_))\n",
    "    mejor_k = range(2, len(sils)+2)[np.argmax(sils)]\n",
    "    n_clusters = N_CLUSTERS if N_CLUSTERS else min(mejor_k, 4)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
    "    cent_pca = pca.transform(kmeans.cluster_centers_)\n",
    "    ax.scatter(cent_pca[:, 0], cent_pca[:, 1], c='red', marker='X', s=200)\n",
    "    ax.set_title(f'Clusters K-Means (K={n_clusters})', fontweight='bold')\n",
    "    plt.savefig('outputs/06_clustering.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    df_temp = df.loc[df[columnas_continuas].dropna().index].copy()\n",
    "    df_temp['cluster'] = clusters\n",
    "    perfil_clusters = df_temp.groupby('cluster')[columnas_continuas].mean()\n",
    "    display(perfil_clusters.round(4))\n",
    "else:\n",
    "    n_clusters = 0\n",
    "    perfil_clusters = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12- Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultados_ml = pd.DataFrame()\n",
    "mejor_modelo = None\n",
    "df_importancia = pd.DataFrame()\n",
    "\n",
    "if variable_objetivo and variable_objetivo in df.columns:\n",
    "    features = [c for c in columnas_continuas if c != variable_objetivo]\n",
    "    if not features:\n",
    "        features = [c for c in columnas_numericas if c != variable_objetivo]\n",
    "    \n",
    "    df_ml = df[features + [variable_objetivo]].dropna()\n",
    "    X = df_ml[features]\n",
    "    y = df_ml[variable_objetivo]\n",
    "    if y.dtype == 'object':\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "    \n",
    "    if tipo_problema == 'clasificacion':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    scaler_ml = StandardScaler()\n",
    "    X_train_s = scaler_ml.fit_transform(X_train)\n",
    "    X_test_s = scaler_ml.transform(X_test)\n",
    "    \n",
    "    print(f'Train: {len(X_train):,} | Test: {len(X_test):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelos\n",
    "if variable_objetivo and tipo_problema == 'clasificacion':\n",
    "    modelos = {\n",
    "        'Logistic': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "    res = []\n",
    "    mejor_acc = 0\n",
    "    for nom, mod in modelos.items():\n",
    "        mod.fit(X_train_s, y_train)\n",
    "        y_pred = mod.predict(X_test_s)\n",
    "        cv = cross_val_score(mod, X_train_s, y_train, cv=5)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        if acc > mejor_acc:\n",
    "            mejor_acc = acc\n",
    "            mejor_modelo = (nom, mod)\n",
    "        res.append({'Modelo': nom, 'Accuracy': acc, 'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                   'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                   'F1': f1_score(y_test, y_pred, average='weighted', zero_division=0), 'CV': cv.mean()})\n",
    "    df_resultados_ml = pd.DataFrame(res).sort_values('Accuracy', ascending=False)\n",
    "    display(df_resultados_ml)\n",
    "\n",
    "elif variable_objetivo and tipo_problema == 'regresion':\n",
    "    modelos = {\n",
    "        'Linear': LinearRegression(),\n",
    "        'Ridge': Ridge(random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    }\n",
    "    res = []\n",
    "    mejor_r2 = -999\n",
    "    for nom, mod in modelos.items():\n",
    "        mod.fit(X_train_s, y_train)\n",
    "        y_pred = mod.predict(X_test_s)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        if r2 > mejor_r2:\n",
    "            mejor_r2 = r2\n",
    "            mejor_modelo = (nom, mod)\n",
    "        res.append({'Modelo': nom, 'R¬≤': r2, 'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)), 'MAE': mean_absolute_error(y_test, y_pred)})\n",
    "    df_resultados_ml = pd.DataFrame(res).sort_values('R¬≤', ascending=False)\n",
    "    display(df_resultados_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "if mejor_modelo and hasattr(mejor_modelo[1], 'feature_importances_'):\n",
    "    imp = mejor_modelo[1].feature_importances_\n",
    "    idx = np.argsort(imp)[::-1]\n",
    "    df_importancia = pd.DataFrame({'Variable': [features[i] for i in idx], 'Importancia': [imp[i] for i in idx]})\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(df_importancia['Variable'], df_importancia['Importancia'], color=plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(df_importancia))))\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f'Feature Importance - {mejor_modelo[0]}', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/07_feature_importance.png', dpi=150)\n",
    "    plt.show()\n",
    "    display(df_importancia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n\n",
    "if mejor_modelo and tipo_problema == 'clasificacion':\n",
    "    y_pred = mejor_modelo[1].predict(X_test_s)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "    ax.set_title(f'Matriz de Confusi√≥n - {mejor_modelo[0]}', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/08_confusion_matrix.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13- Generar Resumen para Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumen = f'''\n",
    "RESUMEN DEL AN√ÅLISIS - {DATASET_NAME}\n",
    "{'='*60}\n",
    "\n",
    "DESCRIPCI√ìN: {DATASET_DESCRIPTION}\n",
    "\n",
    "1. DATASET: {df.shape[0]:,} registros x {df.shape[1]} columnas\n",
    "   - Continuas: {len(columnas_continuas)} | Categ√≥ricas: {len(columnas_categoricas)}\n",
    "   - Objetivo: {variable_objetivo} | Tipo: {tipo_problema}\n",
    "   - Duplicados: {df.duplicated().sum():,}\n",
    "\n",
    "2. CALIDAD: Faltantes: {df.isnull().sum().sum():,} ({porcentaje_faltantes.mean():.2f}%)\n",
    "   Outliers: {total_outliers:,}\n",
    "\n",
    "3. ESTAD√çSTICAS:\n",
    "{estadisticas[[\"mean\", \"std\", \"min\", \"max\"]].to_string() if len(estadisticas) > 0 else \"N/A\"}\n",
    "\n",
    "4. NORMALIDAD:\n",
    "{df_normalidad.to_string() if len(df_normalidad) > 0 else \"N/A\"}\n",
    "\n",
    "5. TOP CORRELACIONES:\n",
    "{df_top_corr.to_string() if len(df_top_corr) > 0 else \"N/A\"}\n",
    "\n",
    "6. TESTS: Variables significativas: {len(vars_sig) if \"vars_sig\" in dir() else 0}\n",
    "\n",
    "7. PCA: Componentes 80%: {n_80} | 95%: {n_95}\n",
    "\n",
    "8. CLUSTERING: {n_clusters} clusters\n",
    "\n",
    "9. ML RESULTADOS:\n",
    "{df_resultados_ml.to_string() if len(df_resultados_ml) > 0 else \"N/A\"}\n",
    "   Mejor: {mejor_modelo[0] if mejor_modelo else \"N/A\"}\n",
    "\n",
    "10. FEATURE IMPORTANCE:\n",
    "{df_importancia.to_string() if len(df_importancia) > 0 else \"N/A\"}\n",
    "'''\n",
    "\n",
    "with open('reports/resumen_analisis_v2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(resumen)\n",
    "print('Guardado: reports/resumen_analisis_v2.txt')\n",
    "print(resumen[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 14- Generar Insights con Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not api_key:\n",
    "    print('Configura ANTHROPIC_API_KEY en .env')\n",
    "else:\n",
    "    print('API Key encontrada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if api_key:\n",
    "    print('Generando insights...')\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    prompt = f'''\n",
    "Eres experto en an√°lisis de datos y ML. Analiza este resumen y genera un REPORTE EJECUTIVO con:\n",
    "\n",
    "1. RESUMEN EJECUTIVO (3 p√°rrafos)\n",
    "2. CALIDAD DE DATOS\n",
    "3. HALLAZGOS CLAVE (5)\n",
    "4. AN√ÅLISIS DE ML\n",
    "5. AN√ÅLISIS PCA/CLUSTERING\n",
    "6. RECOMENDACIONES (5)\n",
    "7. LIMITACIONES\n",
    "8. CONCLUSIONES Y PR√ìXIMOS PASOS\n",
    "\n",
    "CONTEXTO: {DATASET_DESCRIPTION}\n",
    "\n",
    "DATOS:\n",
    "{resumen}\n",
    "\n",
    "Responde en espa√±ol, profesionalmente, usando datos espec√≠ficos.\n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model='claude-sonnet-4-20250514',\n",
    "            max_tokens=6000,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        insights = response.content[0].text\n",
    "        \n",
    "        with open('reports/insights_claude_v2.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(f'REPORTE - {DATASET_NAME}\\n{\"=\"*60}\\n\\n{insights}')\n",
    "        \n",
    "        print('='*60)\n",
    "        print('REPORTE GENERADO')\n",
    "        print('='*60)\n",
    "        print(insights)\n",
    "        print('\\nGuardado: reports/insights_claude_v2.txt')\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('AN√ÅLISIS COMPLETADO')\n",
    "print('='*60)\n",
    "print(f'''\n",
    "{DATASET_NAME}\n",
    "{df.shape[0]:,} x {df.shape[1]}\n",
    "\n",
    "Gr√°ficas generadas en outputs/\n",
    "üìÑ Reportes en reports/\n",
    "\n",
    "M√©tricas:\n",
    "   ‚Ä¢ Outliers: {total_outliers:,}\n",
    "   ‚Ä¢ Clusters: {n_clusters}\n",
    "   ‚Ä¢ Mejor modelo: {mejor_modelo[0] if mejor_modelo else \"N/A\"}\n",
    "   ‚Ä¢ Top variable: {df_importancia.iloc[0][\"Variable\"] if len(df_importancia) > 0 else \"N/A\"}\n",
    "''')\n",
    "print('\\n¬°Completado!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
